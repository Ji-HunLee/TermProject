#import keras for CNN Learning
import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D
from keras.layers.normalization import BatchNormalization

#Libraries for Data Load
from PIL import Image # used for loading images
import imageio
import os

#Image Processing Libraries
import cv2 
import numpy as np
from random import shuffle
from sklearn.model_selection import train_test_split
np.random.seed(1000)

#img_dir = '/content/image'
IMG_SIZE = 224

def label_img(name):
  if naming_dict[name] == '1' :
    return np.array([1, 0])
  elif naming_dict[name] == '-1' :
    return np.array([0, 1])


def load_training_data():
  train_data = []
  batch_size = 0
  for img in os.listdir(img_dir):
    batch_size += 2
    label = label_img(img)
    path = os.path.join(img_dir, img)
    img = Image.open(path)
    img = img.convert('RGB')
    img = img.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)
    train_data.append([np.array(img), label])

    flip_img = Image.open(path)
    flip_img = flip_img.convert('RGB')
    flip_img = flip_img.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)
    flip_img = np.array(flip_img)
    flip_img = np.fliplr(flip_img)
    train_data_flip.append([flip_img, label])

  shuffle(train_data)
  return train_data, batch_size

naming_dict = {}
#f = open('/content/list_attr_celeba.csv', "r")
f = open('/content/drive/My Drive/Colab Notebooks/celeba-dataset/list_attr_celeba.csv', "r")
fileContents = f.read()
fileContents = fileContents.split('\n')

for i in range(len(fileContents)-1):
  fileContents[i] = fileContents[i].split(',')
  naming_dict[fileContents[i][0]] = fileContents[i][21]

print(fileContents[0][21])
train_data, batch_size = load_training_data()

#print(train_data[2][1])

x_train_all = np.empty((batch_size,IMG_SIZE,IMG_SIZE,3))
y_train_all = np.empty((batch_size,2))
print(x_train_all.shape)

for i in range(batch_size):
  x_train_all[i] = train_data[i][0]
  y_train_all[i] = train_data[i][1]
  
x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)

x_train = x_train.astype('float32') / 255.0
x_val = x_val.astype('float32') / 255.0

# sequential model
model = Sequential()

# 1st Convolutional Layer
model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), kernel_initializer='random_uniform', bias_initializer='zeros',strides=(4,4), padding='valid'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
model.add(BatchNormalization())

# 2nd Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(5,5), kernel_initializer='random_uniform', bias_initializer='zeros',strides=(1,1), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))
model.add(BatchNormalization())

# 3rd Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3),kernel_initializer='random_uniform', bias_initializer='zeros', strides=(1,1), padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())

# 4th Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), kernel_initializer='random_uniform', bias_initializer='zeros', strides=(1,1), padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())

# 5th Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(3,3),kernel_initializer='random_uniform', bias_initializer='zeros', strides=(1,1), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))
model.add(BatchNormalization())

model.add(Flatten())
# 1st Dense Layer
model.add(Dense(4096, input_shape=(224*224*3,),kernel_initializer='random_uniform', bias_initializer='zeros'))
model.add(Activation('relu'))
model.add(Dropout(0.4))
model.add(BatchNormalization())

# 2nd Dense Layer
model.add(Dense(4096,kernel_initializer='random_uniform', bias_initializer='zeros'))
model.add(Activation('relu'))
model.add(Dropout(0.4))
model.add(BatchNormalization())

# 3rd Dense Layer
model.add(Dense(1000,kernel_initializer='random_uniform', bias_initializer='zeros'))
model.add(Activation('relu'))
model.add(Dropout(0.4))
model.add(BatchNormalization())

# Output Layer
model.add(Dense(2,kernel_initializer='random_uniform', bias_initializer='zeros'))
model.add(Activation('softmax'))

model.summary()

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

cnn_history = model.fit(x_train, y_train, epochs=30, validation_data=(x_val, y_val))

model.save_weights('AlexNet_modelff_weights.h5')
model.save('AlexNet_modelff.h5')

model_json = model.to_json()
with open("modelff.json", "w") as json_file:
  json_file.write(model_json)
